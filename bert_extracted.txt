When executing a search, Elasticsearch will pick the "best" copy of the data based on the adaptive replica selection formula.
In
The routing parameter can be multi valued represented as a comma
Response time of past requests between the coordinating node and the node containing the copy of the data
Time past search requests
The queue size of the search threadpool on the node containing the data
This can be turned off by changing the dynamic cluster setting cluster.routing.use_adaptive_replica_selection from true to false:
If adaptive replica selection is turned off, searches are sent to the index/indices shards in a round robin fashion between all copies of the data (primaries and replicas).
Individual searches can have a timeout as part of the Request
Searches can be cancelled using standard task cancellation mechanism.
By
The request parameter max_concurrent_shard_requests can be used to control the maximum number of concurrent shard requests the search API will execute for the request. This parameter should be used to protect a single request from overloading a cluster (e.g., a default request will hit all indices in a cluster which could cause shard request rejections if the number of shards per node is
The search API allows you to execute a search query and get back search hits that match the query. The query can either be provided using a simple query string as a parameter, or using a request body.
All search APIs can be applied across multiple indices with support for the multi index syntax.
A search request can be executed purely using a URI by providing request
The parameters allowed in the URI are:
The default field to use when no field prefix is defined within the query.
analyzer
The number of shard results that should be reduced at once on the coordinating node. This value should be used as a protection mechanism to reduce the memory overhead per search request if the potential number of shards in the request can be large.
explain
sort
A search timeout, bounding the search request to be executed within the specified time value and bail with the hits accumulated up to that point when expired. Defaults
The maximum number of documents to collect for each shard, upon reaching which the query execution will terminate
from
size
The search request can be executed with a search DSL, which includes the Query DSL, within its body. Here is an
A search timeout, bounding the search request to be executed within the specified time value and bail with the hits accumulated up to that point when expired. Search requests are canceled after the timeout is reached using the Search Cancellation
from
size
The maximum number of documents to collect for each shard, upon reaching which the query execution will terminate
The number of shard results that should be reduced at once on the coordinating node. This value should be used as a protection mechanism to reduce the memory overhead per search request if the potential number of shards in the request can be large.
Out of the above, the search_type, request_cache and the allow_partial_search_results settings must be passed as query-string
Both HTTP GET and HTTP POST can be used to execute search with body. Since not all clients support GET with body, POST is allowed
terminate_after is always applied after the post_filter and stops the query as well as the aggregation executions when enough hits have been collected on the shard. Though the doc count on aggregations may not reflect the hits.total in the response since aggregations are applied before the post filtering.
The response will not contain any hits as the size was set to 0. The hits.total will be either equal to 0, indicating that there were no matching documents, or greater than 0 meaning that there were at least as many documents matching the query when it was early
The took time in the response contains the milliseconds that this request took for processing, beginning quickly after the node received the query, up until all search related work is done and before the above JSON is returned to the client. This means it includes the time spent waiting in thread pools, executing a distributed search across the whole cluster and gathering all the results.
the object notation allows to specify a custom format
Doc value fields can work on fields that have
* can be used as a wild card, for example:
Date fields can take any date
By default fields are formatted based on a sensible configuration that depends on their mappings:
Allows to collapse search results based on field values. The collapsing is done by selecting only the top sorted document per collapse
define the offset of the first collapsed result
The total number of hits in the response indicates the number of matching documents without
The field used for collapsing must be a single valued keyword or numeric field with doc_values activated
Expand collapse results
the name used for the inner hit section in the response
the number of inner_hits to retrieve per collapse key
the number of concurrent requests allowed to retrieve the inner_hits` per group
See inner hits for the complete list of supported options and the format of the response.
The expansion of the group is done by sending an additional query for each inner_hit request for each collapsed hit returned in the response. This can significantly slow things down if you have too many groups
collapse cannot be used in conjunction with scroll, rescore or search after.
Second level of collapsing is also supported and is applied to inner_hits. For
Pagination of results can be done by using the from and size parameters.
Highlighters enable you to get highlighted snippets from one or more fields in your search results so you can show users where the query matches are.
Highlighting requires the actual content of a field. If the field is not stored (the mapping does not set store to true), the actual _source is loaded and the relevant field is extracted from _source.
The plain highlighter works best for highlighting simple query matches in a single field. To accurately reflect query logic, it creates a tiny in-memory index and re-runs the original query criteria through Lucene’s query execution planner to get access to low-level match information for the current document. This is repeated for every field and every document that needs to be highlighted.
Can assign different weights to matches at different positions allowing for things like phrase matches being sorted above term matches when highlighting a Boosting
To create meaningful search snippets from the terms being queried, the highlighter needs to know the start and end character offsets of each word in the original text. These offsets can be obtained from:
Term
Plain
Plain highlighting for large texts may require substantial amount of time and memory.
Highlighting settings can be set on a global level and overridden at the field level.
boundary_chars A string that contains each boundary character.
When used with the unified highlighter, the sentence scanner splits sentences bigger than fragment_size at the first word boundary next to fragment_size. You can set fragment_size to 0 to never split any sentence.
word Break highlighted fragments at the next word boundary, as determined by Java’s
Only text and keyword fields are highlighted when you use
simple Breaks up text into same-sized
Elasticsearch does not validate that highlight_query contains the search query in any way so it is possible to define it so legitimate query results are not highlighted.
matched_fields Combine matches on multiple fields to highlight a single field.
Specify a highlight query
Combine matches on multiple fields
Highlight using the postings list
Specify a highlight query
You can specify a highlight_query to take additional information into account when
The type field allows to force a specific highlighter
Combine matches on multiple fields
This is only supported by the fvh highlighter
In the following examples, comment is analyzed by the english analyzer and comment.plain is analyzed by the standard analyzer.
The above matches both "run with scissors" and "running with scissors" and would highlight "running" and "scissors" but not "run". If both phrases appear in a large document then "running with scissors" is sorted above "run with scissors" in the fragments list because there are more matches in
The above highlights "run" as well as "running" and "scissors" but
Elasticsearch highlights the fields in the order that they are sent,
None of the highlighters built into Elasticsearch care about the order that the fields are highlighted but
If the number_of_fragments value is set to 0 then no fragments are produced, instead the whole content of the field is returned, and
In the case where there is no matching fragment to highlight, the default is to not return anything.
Highlight using the postings list
Given a query and a text (the content of a document field), the goal of a highlighter is to find the best text fragments for the query, and highlight the query terms in the found fragments. For
How break a text into fragments?
To find the best, most relevant,
The plain highlighter creates an in-memory index from the current token stream, and re-runs the original query criteria through Lucene’s query execution planner to get access to low-level match information for the current text. For more complex queries the original query could be converted to a span query, as span queries can handle phrases more
The goal is to highlight only those terms that participated in generating the hit on the document. For some complex boolean queries, this
FVH and
An example of the work of the unified highlighter
We put the following document into the index:
After doc1 is found as a hit for this query, this hit will be passed to the unified highlighter for highlighting the field content of the document. Since the field content was not indexed either with offsets or term vectors, its raw field value will be analyzed, and in-memory index will be built from the terms that match the query:
"I'll be the only fox in the world for you."and will format with the tags <em> and </em> all matches in this string using the passages’s matchStarts and matchEnds information:
I'll be the <em>only</em> <em>fox</em> in the world for you.This kind of formatted strings are the final result of the highlighter returned to the user.
Allows to configure different boost level per index when searching across more than one indices.
The parent-join and nested features allow the return of documents that have matches in a different scope.
Inner hits can be used by defining an inner_hits definition on a nested, has_child or has_parent query and filter. The structure
If inner_hits is defined on a query that supports it then each search hit will contain an inner_hits json object with the following structure:
from
size
The maximum number of hits to return per
sort
How the inner hits should be sorted per inner_hits.
name
The name to be used for the particular inner hit definition in the response.
Inner hits also supports the following per document features:
Nested inner hits
The nested inner_hits can be used to include nested inner objects as inner hits to a search hit.
The inner hit definition in the nested
An example of a response snippet that could be generated from the above search request:
The
By default the _source is returned also for the hit objects in inner_hits, but this can be
An important default is that the _source returned in hits inside inner_hits is relative to the _nested metadata. So in the above example only the comment part is returned per nested hit and not the entire source of the top level document that contained the comment.
Nested document
If a mapping has multiple levels of hierarchical nested object fields each level can be accessed via dot notated
This indirect referencing is only supported for nested inner hits.
Parent/child inner hits
The inner hit definition like in the nested example.
An example of a response snippet that could be generated from the above search request:
Exclude documents which have a _score less than the minimum specified in min_score:
Each filter and query can accept a _name in its top level definition.
The search response will include for each hit the matched_queries it
Post filter
The post_filter is applied to the search hits at the very end of a search request, after aggregations have already been
Returns the most popular models of red shirts
The main query now finds all shirts by Gucci, regardless of color.
The colors agg returns popular colors for shirts
Controls a preference of the shard copies on which to execute the search. By default, Elasticsearch selects from the available shard copies in an unspecified order, taking the allocation awareness and adaptive replica selection configuration into
The preference is a query string parameter which can be set to:
The operation will be executed only on shards allocated to the local node.
The operation will be executed on shards allocated to the local node
The operation will be executed on nodes with one of the provided node ids (abc
Restricts the operation to the specified shards. (2 and 3 in this case). This preference can be combined with other preferences but it has to appear
Restricts the operation to nodes specified according to the node specification. If suitable shard copies exist on more than one of the selected nodes then the order of preference between these copies is unspecified.
Any value that does not start with _. If two searches both give the same custom string value for their preference and the underlying cluster state does not change then the same ordering of shards will be used for the searches. This does not guarantee that the exact same shards will be used each
The _only_local preference guarantees only to use shard copies on the local node, which is sometimes useful for troubleshooting. All other options do not fully guarantee that any particular shard copies are used in a search, and on a changing index this may mean that repeated searches may yield different results if they are executed on different shard copies which are in different refresh
The query element within the search request body allows to define a query using the Query
Rescoring can help to improve precision by reordering just the top (eg 100 - 500) documents returned by the query and post_filter
The way the scores are combined can be controlled with the score_mode:
Add the original score and the rescore query score.
multiply
Multiply the original score by the rescore query score. Useful for function query rescores.
Average the original score and the rescore query score.
Take the max of original score and the rescore query score.
min
Take the min of the original score and the rescore query score.
The first one gets the results of the query then the second one gets the results of the first,
Script fields can work on fields that are not stored (my_field_name in the above case), and allow to return custom values to be returned (the evaluated value of the script).
Script fields can also access the actual _source document and extract specific elements to be returned from it by using params['_source'].
Note the _source keyword here to navigate the json-like model.
It’s important to understand the difference between doc['my_field'].value and params['_source']['my_field']. The
While a search request returns a single “page” of results, the scroll API can be used to retrieve large numbers of results (or
Scrolling is not intended for real time user requests, but rather for processing large amounts of data, e.g. in order to reindex the contents of one index into a new index with a different configuration.
Client support for scrolling and reindexing
Some of the officially supported clients provide helpers to assist with scrolled searches and reindexing of documents from one index to another:
The result from the above request includes a _scroll_id, which should be passed to the scroll API in order to retrieve the next batch of results.
GET or POST can be used and the URL should not include the index name — this is specified in the original search request instead.
The size parameter allows you to configure the maximum number of hits to be returned with each batch of results. Each call to the scroll API returns the next batch of results until there are no more results left to
The initial search request and each subsequent scroll request each return a _scroll_id. While the _scroll_id may change between requests,
Scroll requests have optimizations that make them faster when the sort order is _doc.
Keeping the search context alive
Normally, the background merge process optimizes the index by merging together smaller segments to create new bigger segments, at which time the smaller segments are deleted. This process continues during
To prevent against issues caused by having too many scrolls
Search context are automatically removed when the scroll timeout has been exceeded.
All search contexts can be cleared with the _all
The id of the slice
The result from the first request returned documents that belong to the first slice (id:
If the number of slices is bigger than the number of shards the slice filter is very slow on the first calls, it has a complexity of O(N) and a memory cost equals to N bits per slice where N is the total number of documents in the shard. After few calls the filter should be cached and subsequent calls should be faster but you should limit the number of sliced query you perform in parallel to avoid the memory explosion.
To avoid this cost entirely it is possible to use the doc_values of another field to do the slicing but the user must ensure that the field has the following properties:
Every document should contain a single value. If a document has multiple values for the specified field, the first value is
The value for each document should be set once when the document is created and
The cardinality of the field should be high. This ensures that each slice gets approximately the same amount of documents.
Pagination of results can be done by using the from and size but the cost becomes prohibitive when the deep pagination is
A field with one unique value per document should be used as the tiebreaker of the sort specification.
The result from the above request includes an array of sort values for each document. These sort values can be used in conjunction with the search_after parameter to start returning results "after" any document in the result list.
search_after is not a solution to jump freely to a random page but rather to scroll many queries in parallel. It is very similar to the scroll API but
One of the questions when executing a distributed search is how many results to retrieve from each shard.
Another question, which relates to the search engine, is the fact that each shard stands on its own. When a query is executed on a specific shard, it does not take into account term frequencies and other search engine information from the other shards.
Also, because of the need to sort the results, getting back a large document
Elasticsearch is very flexible and allows to control the type of search to execute on a per search request
The request is processed in two phases.
Returns the sequence number and primary term of the last modification to each search
Allows you to add one or more sorts on specific fields. Each sort can be reversed as
The sort values for each document returned are also returned as part of the response.
The order option can have the following values:
asc
desc
The order defaults to desc when sorting on the _score, and defaults to asc when sorting on anything else.
min
sum
Use the sum of all values as sort value.
Use the average of all values as sort value.
median
Use the median of all values as sort value.
The default sort mode in the ascending sort order is min — the lowest value is picked. The default sort mode in the descending order is max — the highest value is picked.
Sort mode example usage
In the example below the field price has multiple prices per document.
path
In the below example offer is a field of type nested.
In the below example parent and child fields are of type
Nested sorting is also supported when sorting by scripts and sorting by geo distance.
The missing parameter specifies how docs which are missing the sort field should be treated: The missing value can be set to _last, _first, or a custom value (that will be used for missing docs
If a nested inner object doesn’t match with the nested_filter then a missing value is used.
If any of the indices that are queried doesn’t have a mapping for price then Elasticsearch will handle it as if there was a mapping of type long, with all documents in this index having no value for this field.
distance_type
The following formats are supported in providing the coordinates:
Format in [lon, lat], note, the order of lon/lat here in order to conform with GeoJSON.
Multiple geo points can be passed as an array containing any geo_point
The final distance for a document will then be
When sorting on a field, scores are not
By default operations return the contents of the _source field unless you have used the stored_fields parameter or if the _source field is disabled.
You can turn off _source retrieval by using the _source parameter:
* can be used to load all stored fields from the document.
An empty array will cause only the _id and _type for each hit to be returned,
Stored field values fetched from the document itself are always returned as an array. On the contrary, metadata fields like _routing are never returned as an array.
Script fields can also be automatically detected and used as
_source and version parameters cannot be activated if
Generally the total hit count can’t be computed accurately without visiting all matches, which is costly for queries that match lots of documents. The track_total_hits parameter allows you to control how the total number of hits should be tracked.
When set to true the search response will always track the number of hits that match the query accurately (e.g. total.relation will always be equal to "eq" when track_total_hits is set to true). Otherwise the "total.relation" returned in the "total" object in the search response determines how the "total.value" should be interpreted. A value of "gte" means that the "total.value" is a lower bound of the total hits that match the query and a value of "eq" indicates that "total.value" is the accurate count.
The total number of hits that match the query.
If the total number of his that match the query is greater than the value set in track_total_hits, the total hits in the response will indicate that the returned value is a lower bound:
The /_search/template endpoint allows to use the mustache language to pre render search requests, before they are executed and fill existing templates with template parameters.
For more information on how Mustache
The mustache language is implemented in Elasticsearch as a sandboxed
More template examples
which is rendered as:
which is rendered as:
Concatenating array of values
which is rendered as:
The function also accepts a custom delimiter:
which is rendered as:
Conditional clauses cannot be expressed using the JSON form of the template. Instead, the template must be passed as a string.
Include the gte clause only if line_no.start is specified
Include the lte clause only if line_no.end is specified
The {{#url}}value{{/url}} function can be used to encode a string value in a HTML encoding form as defined in by the HTML specification.
This template can be retrieved by
which is rendered as:
This template can be deleted by
This call will return the rendered template:
status array has been populated with values from the params object.
Pre-registered templates can also be rendered using
The multi search template API allows to execute several search template requests within the same API using the _msearch/template endpoint.
The header part supports the same index, search_type, preference, and routing options as the usual Multi
The body includes a search template body request and supports inline,
Inline search template request
Search template request based on a stored template
The response returns a responses array, which includes the search template response for each search template request matching its order in the original multi search template request.
The search shards api returns the indices and shards that a search request would be executed against.
The index may be a single value, or comma-separated.
routing
preference
Controls a preference of which shard replicas to execute the search request
local
A boolean value whether to read the cluster state locally in order to determine where shards are allocated instead of using the Master node’s cluster state.
The suggest feature suggests similar looking terms based on a provided text by using a suggester. Parts of the suggest feature are
The suggest request part is defined alongside the query part in a
Several suggestions can be specified per
The below suggest response example includes the suggestion response for my-suggest-1 and my-suggest-2. Each suggestion part contains entries. Each entry is effectively a token from the suggest text and contains the suggestion entry text, the original start offset and length in the suggest text and
Each options array contains an option object that includes the suggested text, its document frequency and score compared to the suggest entry text.
To avoid repetition of the suggest text, it is possible to define a global text. In the example below the suggest text is defined globally and applies to the my-suggest-1 and my-suggest-2 suggestions.
The suggest text can in the above example also be specified as suggestion specific option.
In order to understand the format of suggestions, please read the Suggesters page first.
text
field
The field to fetch the candidate suggestions from.
analyzer
The analyzer to analyse the suggest text with.
size
sort
The suggest mode controls what suggestions are included or controls for what suggest text terms, suggestions should be suggested.
Lower cases the suggest text terms after text analysis.
The maximum edit distance candidate suggestions can have in order to be considered as a suggestion.
The number of minimal prefix characters that must match in order be a candidate
Sets the maximum number of suggestions to be retrieved from each individual shard.
The minimal threshold in number of documents a suggestion should appear in.
The maximum threshold in number of documents a suggest text token can exist in order to be included.
Which string distance implementation to use for comparing how similar suggested terms are.
In order to understand the format of suggestions, please read the Suggesters page first.
The term suggester provides a very convenient API to access word alternatives on a per token basis within a certain string distance.
In general the phrase suggester requires special mapping up front to work. The phrase suggester examples on this page need the following mapping to work. The reverse analyzer is used only in the last example.
Once you have the analyzers and mappings set up you can use the phrase suggester in the same spot you’d use the term suggester:
The response contains suggestions scored by the most likely spell correction first.
field
The name of the field used to do n-gram lookups for the language model, the suggester will use this field to gain statistics to score corrections.
Sets max size of the n-grams (shingles) in the field. If the field doesn’t contain n-grams (shingles), this should be omitted or set to
The likelihood of a term being a misspelled even if the term exists in the dictionary.
confidence
The confidence level defines a factor applied to the input phrases score which is used as a threshold for other suggest candidates.
The maximum percentage of the terms considered to be misspellings in order to form a correction. This method accepts a float value in the range [0..1) as a fraction of the actual query terms or a number >=1 as an absolute number of query terms. The default is set to 1.0, meaning only corrections with at most one misspelled term are returned.
size
analyzer
Sets the analyzer to analyze to suggest text with.
Sets the maximum number of suggested terms to be retrieved from each individual shard.
text
Sets up suggestion
Checks each suggestion against the specified query to prune suggestions for which no matching docs exist in the index.
The {{suggestion}} variable will be replaced by the text of each suggestion.
An additional field_name variable has been specified in params and is used by the match query.
All suggestions will be returned with an extra collate_match option indicating whether the generated phrase matched any document.
The phrase suggester supports multiple smoothing models to balance weight between infrequent grams (grams (shingles) are not existing in the index) and frequent grams (appear at least once in the index).
A simple backoff model that backs off to lower order n-gram models if the higher order count is 0 and discounts the lower order n-gram model by a constant factor. The default discount is
laplace
A smoothing model that takes the weighted mean of the unigrams, bigrams, and trigrams based on user supplied weights (lambdas).
The phrase suggester uses candidate generators to produce a list of possible terms per term in the given text.
field
The field to fetch the candidate suggestions from.
size
The suggest mode controls what suggestions are included on the suggestions generated on each shard. All values other than always can be thought of as an optimization to generate fewer suggestions to test on each shard and are not rechecked when combining the suggestions generated on each shard. Thus missing will generate suggestions for terms on shards that do not contain them even if other shards do contain them. Those should be filtered out using confidence. Three possible values can be specified:
The maximum edit distance candidate suggestions can have in order to be considered as a suggestion.
The number of minimal prefix characters that must match in order be a candidate
The minimal threshold in number of documents a suggestion should appear in. This can be specified as an absolute number or as a relative percentage of number of documents. This can improve quality by only suggesting high frequency terms.
The maximum threshold in number of documents in which a suggest text token can exist in order to be included.
The following example shows a phrase suggest call with two generators: the first one is using a field containing ordinary indexed terms, and the second one uses a field that uses terms indexed with a reverse filter (tokens are index in reverse order). This is used to overcome the limitation of the direct generators to require a constant prefix to provide high-performance
pre_filter and post_filter can also be used to inject synonyms after candidates are generated. For instance for the query captain usq we might generate a candidate usa for the term usq, which is a synonym for america. This allows us to present captain america to the user if this phrase scores high
In order to understand the format of suggestions, please read the Suggesters page first.
analyzer
The index analyzer
The search analyzer to
Limits the length of a single input, defaults to 50 UTF-16 code points. This limit is only used at index time to reduce the total number of characters per input string in order to prevent massive inputs from bloating the underlying datastructure. Most use cases won’t be influenced by the default value since prefix completions seldom grow beyond prefixes
You index suggestions like any other
input
weight
The configured weight for a suggestion is returned as
The basic completion suggester query supports the following parameters:
field
size
In case of completion queries spanning more than one shard, the suggest is executed in two phases, where the last phase fetches the relevant documents from shards, implying executing completion requests against a single shard is more performant due to the document fetch overhead when the suggest spans multiple shards. To get best performance for completions, it is recommended to index completions into a single shard
Queries can return duplicate suggestions coming from different documents.
Suggestions that share the longest prefix to the query prefix will be scored
The fuzzy query can take specific fuzzy parameters. The following parameters are supported:
if set to true, transpositions are counted as one change instead of two, defaults to true
Minimum length of the input before fuzzy suggestions are
Minimum length of the input, which is not checked for fuzzy alternatives, defaults
The completion suggester
Regular expressions are dangerous because it’s
The completion suggester considers all documents in the index, but it is often desirable to serve suggestions filtered and/or boosted by some criteria.
To achieve suggestion filtering
Defines a category context
Defines a geo context named location where the categories must be sent with the suggestions.
Defines a geo context named location where the categories are read from the loc
Adding context mappings increases the index size for completion
The category context allows you to associate one or more categories with suggestions at index time.
The mappings are set up like the place_type fields above.
If the mapping had a path then the following index request would be enough to add the categories:
If context mapping references another field and the categories are explicitly indexed, the suggestions are indexed with both set of categories.
Suggestions can be filtered by one or more categories.
If multiple categories or category contexts are set on the query they are merged as a disjunction.
Suggestions with certain categories can be boosted higher than
The context query filter suggestions associated with categories cafe and restaurants and boosts the suggestions associated with restaurants by a factor of 2
In addition to accepting category values, a context query can be composed of multiple category context clauses.
boost
The factor by which the score of the suggestion should be boosted, the score is computed by multiplying the boost with the suggestion weight,
prefix
Whether the category value should be treated as a prefix or not.
If a suggestion entry matches multiple contexts the final score is computed as the maximum score produced by any matching contexts.
A geo context allows you to associate one or more geo points or geohashes with suggestions at index time.
Internally, geo points are encoded as geohashes with the specified precision.
In addition to the path setting, geo context mapping accepts the following settings:
precision
This defines the precision of the geohash to be indexed and can be specified as a distance value
The index time precision setting sets the maximum geohash precision that can be used at query time.
geo contexts can be explicitly set with suggestions or be indexed from a geo point field in the document via the path parameter,
Suggestions can be filtered and boosted with respect to how close they are to one or more geo points. The following filters suggestions that fall within the area represented by the encoded geohash of a geo point:
If multiple categories or category contexts are set on the query they are merged as a disjunction.
Suggestions that are within an area represented by a geohash can also be boosted higher than others, as shown by the following:
The context query filters for suggestions that fall under the geo location represented by a geohash of (43.662, -79.380) with a precision of 2 and boosts suggestions that fall under the geohash representation of (43.6624803, -79.3863353) with a default precision of 6 by a factor of 2
If a suggestion entry matches multiple contexts the final score is computed as the maximum score produced by any matching contexts.
In addition to accepting context values, a context query can be composed of multiple context clauses.
boost
The factor by which the score of the suggestion should be boosted, the score is computed by multiplying the boost with the suggestion weight,
precision
The precision of the geohash to encode the query geo point. This can be specified as a distance value (5m,
Accepts an array of precision values at which neighbouring geohashes should be taken into account. precision value can be a distance value (5m,
Returning the type of the suggester
Sometimes you need to know the exact type of a suggester in order to parse its results. The typed_keys parameter can be used to change the suggester’s name in the response so that it will be prefixed by its type.
Considering the following example with two suggesters term and phrase:
In the response, the suggester names will be changed to respectively term#my-first-suggester and phrase#my-second-suggester, reflecting the types of each suggestion:
The multi search API allows to execute several search requests within the same API.
The format of the request is similar to the bulk API format and makes use of the newline
NOTE: the final line of data must end with a newline character \n.
The header part includes which index / indices to search on, the search_type, preference, and routing. The body includes the typical search body request (including the query,
Note, the above includes an example of an empty header (can
The response returns a responses array, which includes the search response and status code for each search request matching its order in the original multi search request.
The endpoint allows to also search against an index/indices in the URI itself, in which case it will be used as the default unless explicitly defined otherwise in the header. For example:
The above will execute the search against the twitter index for all the requests that don’t define an index, and the last one will be executed against the twitter2 index.
The request parameter max_concurrent_shard_requests can be used to control the maximum number of concurrent shard requests the each sub search request will execute. This parameter should be used to protect a single request from overloading a cluster (e.g., a default request will hit all indices in a cluster which could cause shard request rejections if the number of shards per node is high).
The count API allows to easily execute a query and get the number of matches for that query. It can be executed across one or more indices. The query can either be provided using a simple query string as a parameter, or using the Query DSL defined within the request body.
The query being sent in the body must be nested in a query key,
The count API can be applied to multiple indices.
When executing count using the query parameter q, the query passed is a query string using Lucene query parser. There are additional parameters that can be passed:
The default field to use when no field prefix is defined within the query.
analyzer
The maximum count for each shard, upon reaching which the query execution will terminate
The count can use the Query DSL within its body in order to express the query that should be executed. The body content can also be passed as a REST parameter named
Both HTTP GET and HTTP POST can be used to execute count with body. Since not all clients support GET with body, POST is allowed
The count operation is broadcast across all shards.
The routing value (a
The validate API allows a user to validate a potentially expensive query without executing it.
When
The default field to use when no field prefix is defined within the query.
analyzer
The query may also be sent in the request body:
The query being sent in the body must be nested in a query key,
An explain parameter can be specified to get more detailed information about why a query failed:
responds
The explain api computes a score explanation for a query and a specific document. This can give useful feedback whether a document matches or didn’t match a specific query.
routing
Controls the routing in the case the routing was used during
preference
source
Allows the data of the request to be put in the query string of the url.
q
The default field to use when no field prefix is defined within the query.
analyzer
The analyzer name to be used when analyzing the query string.
The
The output from the Profile API is
Setting the top-level profile parameter to true will enable profiling for the search
A profile is returned for each shard that participated in the response, and is identified by a unique ID
Each profile contains a section which holds details about the query execution
Each profile has a single time representing the cumulative rewrite time
Each profile also contains a section about the Lucene Collectors which run the search
Each profile contains a section which holds the details about the aggregation execution
Because a search request may be executed against one or more shards in an index, and a search may cover one or more indices, the top level element in the profile response is an array of shard
The profile itself may consist of one or more "searches", where a search is a query executed against the underlying Lucene index. Most search requests submitted by the user will only execute a single search against the Lucene
Inside each search object there will be two arrays of profiled
The details provided by the Profile API directly expose Lucene class names and concepts, which means that complete interpretation of the results require fairly advanced knowledge of Lucene. This page attempts to give a crash-course in how Lucene executes queries so that you can use the Profile API to successfully diagnose and debug queries,
query Section
The query section contains detailed timing of the query tree executed by Lucene on a particular shard.
Based on the profile structure, we can see that our match query was rewritten by Lucene into a BooleanQuery with two clauses (both holding a TermQuery). The type field displays the Lucene class name, and often aligns with the equivalent name in Elasticsearch. The description field displays the Lucene explanation text for the query, and is made available to help differentiating between parts of your query (e.g. both message:search and message:test are TermQuery’s and would appear identical otherwise.
The time_in_nanos field shows that this query took ~1.8ms for the entire BooleanQuery to execute. The recorded time is inclusive of all children.
The breakdown field will give detailed stats about how the time was spent,
The breakdown component lists detailed timing statistics about low-level
Timings are listed in
The meaning of the stats
A
This parameter shows how long it takes to build a Scorer for the query.
The Lucene method next_doc returns Doc ID of the next document matching the query. This statistic shows the time it takes to determine which document is the next match, a process that varies considerably depending on the nature of the query. Next_doc is a specialized form of advance() which is more convenient for many queries in Lucene. It is equivalent to
advance
advance is the "lower level" version of next_doc: it serves the same purpose of finding the next matching doc, but requires the calling query to perform extra tasks such as identifying and moving past skips,
matches
Some queries,
score
This records the time taken to score a particular document via its Scorer
Records the number of invocations of the particular method. For
We see a single collector named SimpleTopScoreDocCollector wrapped
It should be noted that Collector times are independent from the Query times. They are calculated, combined, and normalized independently! Due to the nature of Lucene’s execution, it is impossible to "merge" the times from the Collectors into the Query section, so they are displayed in separate portions.
A collector that scores and sorts documents.
A collector that only counts the number of documents that match the query, but does not fetch the source. This is seen when size: 0 is specified
A collector that terminates search execution after n matching documents have been found.
A collector that only returns matching documents that have a score greater than n. This is seen when the top-level parameter min_score has been specified.
A collector that halts execution after a specified period of time. This is seen when a timeout top-level parameter has been specified.
A collector that Elasticsearch uses to run aggregations against the query scope.
A collector that executes an aggregation against the global query scope, rather than the specified query.
rewrite
All queries in Lucene
The rewriting process is complex and difficult to display, since queries can change drastically.
To demonstrate a slightly more complex query and the associated results, we can profile the following query:
The Collector tree is
A special note needs to be made about the MultiTermQuery class of queries. This includes wildcards, regex, and fuzzy queries. These queries emit very verbose responses, and are not overly structured.
Essentially, these queries rewrite themselves on a per-segment basis.
The
From the profile structure we can see that the my_scoped_agg is internally being run as a LongTermsAggregator (because the field it is aggregating, likes, is a numeric field).
The time_in_nanos field shows the time executed by each aggregation, and is inclusive of all children. While the overall time is useful, the breakdown field will give detailed stats about how the time was spent.
The breakdown component lists detailed timing statistics about low-level
Timings are listed in
The meaning of the stats
collect
This represents the cumulative time spent in the collect phase of the aggregation. This is where matching documents are passed to the aggregation and the state of the aggregator is updated based on the information contained in the documents.
This represents the time spent creating the shard level results of the aggregation ready to pass back to the reducing node after the collection of documents is finished.
reduce
This is not currently used and will always report
Records the number of invocations of the particular method. For
There are also cases where special Lucene optimizations are disabled, since they are not amenable to profiling. This could cause some queries to report larger relative times than their non-profiled counterparts, but in general should not have a drastic effect compared to other components in the profiled query.
Profiling of the reduce phase of aggregation is currently not available
The field capabilities API allows to retrieve the capabilities of fields among multiple indices.
The field capabilities API by default executes on all indices:
The request can also be restricted to specific indices:
fields
The field capabilities API returns the following information per
Whether this field is indexed for search on all indices.
Whether this field can be aggregated on all indices.
indices
The list of indices where this field has the same type, or null if all indices have the same type for the field.
The list of indices where this field is not searchable, or null if all indices have the same definition for the field.
The list of indices where this field is not aggregatable, or null if all indices have the same definition for the field.
The field rating is defined as a long in index1 and index2 and as a keyword in index3 and index4.
The field title is defined as text in all indices.
The ranking evaluation API is experimental and may be changed or removed completely in a future release, as well as change in non-backwards compatible ways on minor versions updates. Elastic will take a best effort approach to fix any issues, but experimental features are not subject to the support SLA of official GA features.
The ranking evaluation API allows to evaluate the quality of ranked search results over a set of typical search queries. Given this set of queries and a list of manually rated documents, the _rank_eval endpoint calculates and returns typical information retrieval metrics like mean reciprocal rank, precision or discounted cumulative gain.
Search quality evaluation starts with looking at the users of your search application, and the things that they are searching
The challenge for search engineers is to tweak this translation process from user entries to a concrete query in such a way, that the search results contain the most relevant information with respect to the users information need. This can only be done if the search result quality is evaluated constantly across a representative test suite of typical user queries, so that improvements in the rankings for one particular query doesn’t negatively effect the ranking for other types of queries.
a collection of documents you want to evaluate your query performance against, usually one or more indices
a collection of typical search requests that users enter into your system
a set of document ratings that judge the documents relevance with respect to a search request+
The ranking evaluation API provides a convenient way to use this information in a ranking evaluation request to calculate different search evaluation metrics. This gives a first estimation of your overall search quality and give you a measurement to optimize against when
Ranking evaluation request structure
In its most basic form, a request to the _rank_eval endpoint has two sections:
a set of typical search requests, together with their provided ratings
The request section contains several search requests typical to
the search requests id, used to group result details later
a list of document ratings, each entry containing the documents _index and _id together with the rating of the documents relevance with regards to this search request
A document rating can be any integer value that expresses the relevance of the document on a user defined scale.
Template based ranking evaluation
As an alternative to having to provide a single query per test request, it is possible to specify query templates in the evaluation request and later refer to them. Queries with similar structure that only differ in their parameters don’t have to be repeated all the time in the requests
a reference to a previously defined template
The metric section determines which of the available evaluation metrics is going to be used.
This metric measures the number of relevant results in the top k search
P@k works well as a simple evaluation metric that has the benefit of being easy to understand and explain. Documents in the collection need to be rated either as relevant or irrelevant with respect to the current query. P@k does not take into account where in the top k results the relevant documents occur, so a ranking of ten results that contains one relevant result in position 10 is equally good as a ranking of ten results that contains one relevant result in position
The precision metric takes the following optional parameters
For every query in the test suite, this metric calculates the reciprocal of the rank of the first relevant document.
The mean_reciprocal_rank metric takes the following optional parameters
In contrast to the two metrics above, discounted cumulative gain takes both, the rank and the rating of the search results,
The assumption is that highly relevant documents are more useful for the user when appearing at the top of the result list.
normalize
It is based on the assumption of a cascade model of search, in which a user scans through ranked search results in order and stops at the first document that satisfies the information need.
The metric models the expectation of the reciprocal of the position at which a user stops reading through the result list.
The response of the _rank_eval endpoint contains the overall calculated result for the defined quality metric, a details section with a breakdown of results for each query in the test suite and an optional failures section that shows potential errors of individual queries. The response has the following format:
the overall evaluation quality calculated by the defined metric
the details section contains one entry for every query in the original requests
the unrated_docs section contains an _index and _id entry for each document in the search result for this query that didn’t have a ratings value. This can be used to ask the user to supply ratings for these documents
the hits section shows a grouping of the search results with their supplied rating
